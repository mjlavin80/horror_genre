{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In March 2016, I did a minimalistic web-scrape of the _Oxford English Dictionary_ database for terms and years. The _OED_ has much more data than this, but I wanted a down-and-dirty alternative to the list of 10,5000 dictiojnary.com terms and years of origin Ted Underwood and Jordan Sellers used in \"The Emergence of Literary Diction.\" I wanted to cross-validate their results and/or see if a larger term set would change anything. The _OED_ is a generally bette rsource than dictionary.com, but the results will depend heavily on how I choose to normalize their more idiosyncratic (humanistic?) data. I would like to parse the set down to usable, ratio-friendly pairs for term and date of earliest known use. I am not aiming at perfection here, as I think that would be a mistake. Instead, I'm shooting for (1) transparency and (2) a method for excluding data that are likely to increase errors or \"statistical noise.\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('oed_data.db')\n",
    "c = conn.cursor()\n",
    "rows = c.execute('SELECT term, GROUP_CONCAT(year) FROM dictionary WHERE year !=\" \" GROUP BY term').fetchall()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'Arry\", ' 1874'),\n",
       " (\"'Namgis\", ' 1966'),\n",
       " (\"'Sblood\", ' 1598'),\n",
       " (\"'Sbobs\", ' 1694'),\n",
       " (\"'Sbodikins\", ' 1677'),\n",
       " (\"'Sbores\", ' 1640'),\n",
       " (\"'Sbud(s\", ' 1676'),\n",
       " (\"'Sdeath\", ' 1606'),\n",
       " (\"'Sdeynes\", ' 1616'),\n",
       " (\"'Sflesh\", ' 1705')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspecting the first ten rows here, we see a few immediate issues. We will want to convert all to lowercase, account for the (s notation (which indicates a term's plural variant), and remove leading apostrophes (for consistency with our already tokenized dataset). Further, we can note that dates are string formations with a leading space. The reason for this is clear if we look a little further down the row list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"'low\", ' a1382'), (\"'magine\", ' 1530'), (\"'mong\", ' ?c1200'), (\"'mongst\", ' 1567'), (\"'n\", ' 1678, 1828'), (\"'n'\", ' 1858'), (\"'ndrangheta\", ' 1978'), (\"'neath\", ' a1500'), (\"'nother\", ' a1635'), (\"'nough\", ' a1618')] [('Abaza', ' 1693'), ('Abba', ' OE'), ('Abbasid', ' 1664'), ('Abbe', ' 1876'), ('Abbevillian', ' 1783')] [('ˌultracytoˈchemistry', ' 1963'), ('ˌultrafilˈtration', ' 1908'), ('ˌultramicroˈscopic', ' 1870'), ('ˌultraˈcold', ' 1967'), ('ˌunder-coˈrrect', ' 1831'), ('ˌunder-differentiˈation', ' 1953'), ('ˌunder-diˈspersion', ' 1935'), ('ˌunder-exˈpose', ' 1890'), ('ˌunder-occuˈpation', ' 1961'), ('ˌunder-proˈficient', ' 1703'), ('ˌunder-proˈportion', ' 1813'), ('ˌunder-proˈportioned', ' 1689'), ('ˌunder-ˈargue', ' 1645'), ('ˌunder-ˈcapitalled', ' 1794'), ('ˌunder-ˈenter', ' 1692'), ('ˌunder-ˈestimate', ' 1812'), ('ˌunder-ˈfurnish', ' 1694'), ('ˌunder-ˈhorsing', ' 1839'), ('ˌunder-ˈmeasure', ' 1682'), ('ˌunder-ˈmeated', ' 1653')]\n"
     ]
    }
   ],
   "source": [
    "print(rows[50:60], rows[310:315], rows[-100:-80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing entries like a1382, ?c1200, and OE when I originally scraped the _OED_, I stored date values as unicode strings. The method I plan to apply to my data won't be able to differentiate between homonyms, whereas the _OED_ does, so I will need to account for repeats in this set. Finally, there's a fair amount of even messier data at the end of the list because the _OED_ search returns separate entries for data for prefixes, and uses special characters to offset the stem. For example, the entry _ˌultracytoˈchemistry_ means the prefix _ultracyto_ is first known to have been used with the ending _chemistry_ in 1963."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('arry', ' 1874'), ('namgis', ' 1966'), ('sblood', ' 1598'), ('sbobs', ' 1694'), ('sbodikins', ' 1677'), ('sbores', ' 1640'), ('sbuds', ' 1676'), ('sdeath', ' 1606'), ('sdeynes', ' 1616'), ('sflesh', ' 1705'), ('sfoot', ' 1602'), ('sheart', ' c1596'), ('slid', ' 1606'), ('slife', ' a1634'), ('slight', ' 1600'), ('slud', ' 1606'), ('snails', ' 1599'), ('sneaks', ' 1602'), ('sniggers', ' 1633'), ('snigs', ' a1643'), ('snowns', ' 1594'), ('sprecious', ' 1631'), ('swill', ' 1602'), ('arf', ' 1854'), ('at', ' a1300'), ('burb', ' 1977'), ('cause', ' a1513'), ('cep', ' 1851'), ('cept', ' 1851'), ('chute', ' 1920')] [('ultracytochemistry', ' 1963'), ('ultrafiltration', ' 1908'), ('ultramicroscopic', ' 1870'), ('ultracold', ' 1967'), ('under-correct', ' 1831'), ('under-differentiation', ' 1953'), ('under-dispersion', ' 1935'), ('under-expose', ' 1890'), ('under-occupation', ' 1961'), ('under-proficient', ' 1703'), ('under-proportion', ' 1813'), ('under-proportioned', ' 1689'), ('under-argue', ' 1645'), ('under-capitalled', ' 1794'), ('under-enter', ' 1692'), ('under-estimate', ' 1812'), ('under-furnish', ' 1694'), ('under-horsing', ' 1839'), ('under-measure', ' 1682'), ('under-meated', ' 1653')]\n"
     ]
    }
   ],
   "source": [
    "def term_normalize(mytuple):\n",
    "    word = [i for i in mytuple[0].lower() if i.isalpha() or i==\"-\"]\n",
    "    word = [i for i in word if i.encode('unicode_escape') != b'\\\\u02c8' and i.encode('unicode_escape') != b'\\\\u02cc']\n",
    "    word = ''.join(word)\n",
    "    new_tuple = (word, mytuple[1])\n",
    "    return new_tuple\n",
    "#lowercase all and drop leading apostrophe and other punctuation (keep hyphens)\n",
    "new_rows = []\n",
    "for i in rows:\n",
    "        a = term_normalize(i)\n",
    "        new_rows.append(a)\n",
    "print(new_rows[:30], new_rows[-100:-80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've done some basic normalization of the terms, we can begin to tackle the year list. We'll also purge homonyms in this block of code, as we'll want to preserve the earliest date for each type. First, let's inspect the range of year strings by converting every number to a 1 and grouping the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1717\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "years_lump = [''.join([\"1\" if s.isdigit() else s for s in i[1]]) for i in new_rows]\n",
    "year_type_counts = Counter(years_lump)\n",
    "print(len(year_type_counts.most_common()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the code above suggests is that, if we convert every digit in our data to a '1', we still have 1717 forms to work with. Let's took a loo why this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' 1111', 190674), (' a1111', 13938), (' c1111', 12523), (' 1111, 1111', 8526), (' ?1111', 2109), (' ?a1111', 1616), (' c1111, 1111', 1532), (' 1111-1', 1327), (' a1111, 1111', 1327), (' OE', 1164), (' 1111-11', 1081), (' ?c1111', 1030), (' c111', 922), (' 1111, 1111, 1111', 862), (' 11..', 568), (' eOE', 560), (' 1111, a1111', 392), (' c1111, c1111', 342), (' c1111, 1111, 1111', 285), (' c1111, a1111', 280), (' ?a1111, 1111', 261), (' a1111, a1111', 225), (' 1111, 1111, 1111, 1111', 215), (' ?1111, 1111', 214), (' a1111, c1111', 204), (' a1111, 1111, 1111', 187), (' 1111, c1111', 169), (' a1111-11', 163), (' a111', 154), (' c111, 1111', 147), (' OE, 1111', 142), (' 111', 140), (' ?c1111, 1111', 139), (' eOE, 1111', 119), (' c1111, 1111, 1111, 1111', 90), (' c111, c1111', 89), (' 1111-1, 1111', 88), (' 1111-11, 1111', 75), (' 1111, 1111, 1111, 1111, 1111', 75), (' c1111, c1111, 1111', 74), (' 1111, ?1111', 71), (' lOE', 67), (' a1111, 1111, 1111, 1111', 59), (' 1111-1111', 58), (' 11.., 1111', 58), (' c1111, a1111, 1111', 58), (' 1111, c1111, 1111', 56), (' ?c1111, c1111', 51), (' c111, c111', 51), (' c1111, ?1111', 51), (' OE, c1111', 49), (' OE, a1111', 48), (' 1111, a1111, 1111', 47), (' c111, a1111', 45), (' a1111, a1111, 1111', 44), (' eOE, OE', 44), (' ?1111-1', 44), (' a1111, c1111, 1111', 44), (' a111, 1111', 43), (' c1111, ?a1111', 39), (' ?a1111, 1111, 1111', 39), (' eOE, a1111', 39), (' c111, 1111, 1111', 37), (' c1111, 1111, 1111, 1111, 1111', 37), (' ?c1111, a1111', 36), (' 1111, 1111, 1111, 1111, 1111, 1111', 34), (' a1111, ?a1111', 33), (' 1111, 1111-1', 32), (' eOE, c1111', 31), (' OE, OE', 31), (' c1111, c1111, 1111, 1111', 31), (' OE, 1111, 1111', 30), (' c1111-11', 30), (' 1111, 1111-11', 29), (' 1111, 1111, a1111', 29), (' a1111, ?1111', 28), (' ?a1111, ?a1111', 28), (' ?c1111, 1111, 1111', 26), (' ?a1111, c1111', 26), (' c1111, a1111, 1111, 1111', 25), (' 111.', 24), (' 1111, ?a1111', 24), (' c111, c1111, 1111', 24), (' a1111-11, 1111', 23), (' eOE, 1111, 1111', 23), (' c1111, ?c1111', 23), (' ?a1111, a1111', 22), (' 1111, 1111, 1111, 1111, 1111, 1111, 1111', 21), (' ?1111, 1111, 1111', 20), (' a1111, 1111, 1111, 1111, 1111', 20), (' 1111-1, 1111, 1111', 20), (' c1111, 1111-11', 19), (' a1111-1111', 18), (' c111, 1111, 1111, 1111', 18), (' a111, c1111', 18), (' eOE, eOE', 17), (' a1111, a1111, 1111, 1111', 17), (' a1111, c1111, 1111, 1111', 17), (' ?c1111, c1111, 1111', 16), (' c1111, c1111, c1111', 16)]\n"
     ]
    }
   ],
   "source": [
    "print(year_type_counts.most_common()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249088 249088\n",
      "217135\n"
     ]
    }
   ],
   "source": [
    "print(len(new_rows), len(years_lump))\n",
    "print(year_type_counts.most_common()[0][1]+year_type_counts.most_common()[1][1]+year_type_counts.most_common()[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the types _year_, a+_year_, and c+_year_, we cover 217135 out of 249088 entries. _a_ represents \"approximately\" and _c_ stands for is circa, so, in both cases, for convenience, we can drop the letter qualifiers and assume they're reasonable on track (and we could run our analysis on non-qualified data and see if it produces a significantly different result). A few more generalizations:\n",
    "\n",
    "1. \"?\" characters tend to precede _year_ or a+_year_/c+_year_\n",
    "2. Most years in the set have four digits, suggesting that post-1100 terms will greatly outnumber pre-1100 terms in this set.\n",
    "3. Every data field seems to be begin with an empty space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, the _OED_ has entries with multiple variants of the same term sharing a single date of origin, and other entries with multiple variants of the same term and multiple dates for each variant. We can locate these entries by searching for a comma in the year field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"'long\", ' 1488, 1663'), (\"'n\", ' 1678, 1828'), ('-ie', ' 1727, 1941'), ('-in', ' 1881, 1960'), ('-ion', ' 1856, 1930'), ('-onium', ' 1858, 1987'), ('-some', ' a1400, 1921'), ('-y', ' c1430, 1850, 1941'), ('ABC', ' c1325, 1611, 1868'), ('ATP', ' 1939, 1971'), ('Abelian', ' ?1609, 1846'), ('Abraham', ' c1300, ?1592'), ('Actaeon', ' 1567, 1582'), ('Adam', ' OE, 1846, 1983'), ('Adam and Eve', ' 1789, 1925'), ('Adamish', ' 1821, 1838'), ('Addisonian', ' 1789, 1885'), ('Ahmadiyya', ' 1836, 1902'), ('Albanian', ' c1400, 1565, ?1569, 1689'), ('Albert', ' 1740, 1840, 1874')]\n"
     ]
    }
   ],
   "source": [
    "serial_dates = [r for r in rows if \",\" in r[1]]\n",
    "print(serial_dates[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data are here because the _OED_ is interested in _senses_ of words, not just tokens. _Albanian_ meaning \"of or relating to Scotland or its people\" came into usage around 1565 (according to the data) and _Albanian_ meaning \"A native or inhabitant of Albania, a country once located in the eastern Caucasus, in the regions that are now Azerbaijan and the southern part of the Russian Republic of Dagestan\" can be dated circa 1400. For all of these, I want to use the earliest possible date (and/or remove them from consideration). Note that our lists seem to be ordered by earliest to latest date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20813\n"
     ]
    }
   ],
   "source": [
    "print(len(serial_dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with these as a group will help us normalize another 20,000 terms. Next up, we have date ranges. We can locate these by searching for a hyphen in the date field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('-gon', ' 1867-78'), ('Americanized', ' 1811-12'), ('Bartholomew', ' 1552-3'), ('Cercaria', ' 1836-9'), ('Clypeˈaster', ' 1836-9'), ('Conˈcordium', ' 1841-3'), ('Cottonian', ' 1700-1, 1846'), ('Crestmarine', ' 1565-73'), ('Cydippe', ' 1835-6'), ('Decapoda', ' 1835-6'), ('Docetae', ' 1818-21'), ('Donegal', ' 1903-4'), ('Easter duty', ' 1598-9'), ('Easterling', ' 1378-9'), ('Eledone', ' 1835-6'), ('Elsan', ' 1939-40'), ('Eucharistize', ' 1714-7'), ('Finnish', ' 1789-96'), ('Fourierism', ' 1841-4'), ('Fructidor', ' 1793-97')]\n"
     ]
    }
   ],
   "source": [
    "date_ranges = [r for r in rows if \"-\" in r[1]]\n",
    "print(date_ranges[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not fail to notice that many of our date ranges are mixed in with serialized lists of dates. We'll have to address this is our normalization code at the end. But, for now, we should also pay attention to the fact that date ranges tend to include only the digits that vary from start to end date. 1867-78 means 1867-1878, and 1836-9 means 1836-1839. These blocks have non-standard lengths, so we'll have to be careful when handling them. We could also use the size of the range to forecaset a certain level of certainty. 1855-56 is quite precise, overall, whereas 1000-1400 would be pretty vague and especially problematic if we're trying to separate terms that are most likely Germanic from terms that are most likely Latinate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have some date fields with no numbers at all. We can use the 'years_lumped' list to inspect them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' OE', 1164), (' eOE', 560), (' lOE', 67), (' eOE, OE', 44), (' OE, OE', 31), (' eOE, eOE', 17), (' eOE, OE, OE', 4), (' eOE, eOE, eOE, eOE', 2), (' eOE, lOE', 2), (' eOE, eOE, eOE', 2), (' OE, lOE', 2), (' lOE, lOE', 1), (' OE, OE, OE', 1), (' eOE, eOE, eOE, OE', 1), (' eOE, OE, OE, OE', 1), (' eOE, eOE, OE, OE', 1), (' eOE, eOE, OE', 1)]\n"
     ]
    }
   ],
   "source": [
    "years_all_letters = [i for i in years_lump if '1' not in i]\n",
    "print(Counter(years_all_letters).most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OE stands for Old English, where eOE is 'early Old English', and 'lOE' is late Old English. So, any time we find a purely textual entry, we should designate it Germanic. Further, if we have a serial data field with OE, eOE, or lOE in it, we should consider that term Germanic as well. Note that, in the following code, a list with 'OE' or its variants almost always has the 'OE' value in the first position. \n",
    "\n",
    "Note: even accounting for OE values, Latinate terms are still going to outnumber Germanic terms significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3010\n"
     ]
    }
   ],
   "source": [
    "years_with_oe = [i for i in years_lump if 'oe' in i.lower()]\n",
    "print(len(years_with_oe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try putting together what's we've seen so far. First, let's try for a normalization script that reduces every date to \"Latin\", \"Germanic\", or \"Neologism\" (for post-1700 data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "terms_and_origins = []\n",
    "exceptions = []\n",
    "for h,i in new_rows:\n",
    "    quals = []\n",
    "    if 'oe' in i.lower():\n",
    "        #the following code is designed to make sure any qualifiers (a, c, ?) are directly before or after oe, eoe, or loe\n",
    "        pattern = \"...oe.|...oe|..oe.|..oe|.oe.|oe.|.oe|oe\"\n",
    "        quals_test = re.search(pattern, i.lower())\n",
    "        if quals_test:\n",
    "            if \"a\" in quals_test.group(0):\n",
    "                quals.append(\"a\")\n",
    "            if \"c\" in quals_test.group(0):\n",
    "                quals.append(\"c\")\n",
    "            if \"?\" in quals_test.group(0):\n",
    "                quals.append(\"?\")\n",
    "        #750 is just a placeholder that will always resolve to 'Germanic\" in the code below.\n",
    "        row = [h, i, 750, quals]\n",
    "    else:      \n",
    "        if i[0] == \" \":\n",
    "            \n",
    "            if i[1].isdigit():\n",
    "                #means first char after space is a number\n",
    "                #test for 4, 3,2, 1 digits\n",
    "                for z in range(5, 1, -1):\n",
    "                    if i[1:z].isdigit():\n",
    "                        try:\n",
    "                            if i[z+1] == \"?\":\n",
    "                                quals.append(\"?\")\n",
    "                        except:\n",
    "                            pass\n",
    "                        year = i[1:z]\n",
    "                        break\n",
    "                row = [h,i,year,quals]\n",
    "                \n",
    "            else:\n",
    "                #match letter or ?letter\n",
    "                if i[2].isdigit():\n",
    "                    quals.append(i[1])\n",
    "                    #test for 4, 3,2, 1 digits\n",
    "                    for z in range(6, 3, -1):\n",
    "                        if i[2:z].isdigit():\n",
    "                            try:\n",
    "                                if i[z+1] == \"?\" or i[1] == \"?\":\n",
    "                                    quals.append(\"?\")\n",
    "                            except:\n",
    "                                pass\n",
    "                            year = i[2:z]\n",
    "                            break\n",
    "                    row = [h,i,year,quals]\n",
    "                else:\n",
    "                    #by def should be ?c or ?a\n",
    "                    quals.append(i[1])\n",
    "                    quals.append(i[2])\n",
    "                    #test for 4, 3,2, 1 digits\n",
    "                    for z in range(7, 4, -1):\n",
    "                        if i[3:z].isdigit():\n",
    "                            try:\n",
    "                                if i[z+1] == \"?\":\n",
    "                                    quals.append(\"?\")\n",
    "                            except:\n",
    "                                pass\n",
    "                            year = i[3:z]\n",
    "                            break\n",
    "                    row = [h,i,year,quals]\n",
    "    if int(row[2]) < 1100:\n",
    "        row.append(\"germ\")\n",
    "    elif int(row[2]) > 1100 and int(row[2]) < 1700:\n",
    "        row.append(\"lat\")\n",
    "    else:\n",
    "        row.append(\"neo\")\n",
    "    terms_and_origins.append(row) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " 249088,\n",
       " [['arry', ' 1874', '1874', [], 'neo'],\n",
       "  ['namgis', ' 1966', '1966', [], 'neo'],\n",
       "  ['sblood', ' 1598', '1598', [], 'lat'],\n",
       "  ['sbobs', ' 1694', '1694', [], 'lat'],\n",
       "  ['sbodikins', ' 1677', '1677', [], 'lat'],\n",
       "  ['sbores', ' 1640', '1640', [], 'lat'],\n",
       "  ['sbuds', ' 1676', '1676', [], 'lat'],\n",
       "  ['sdeath', ' 1606', '1606', [], 'lat'],\n",
       "  ['sdeynes', ' 1616', '1616', [], 'lat'],\n",
       "  ['sflesh', ' 1705', '1705', [], 'neo'],\n",
       "  ['sfoot', ' 1602', '1602', [], 'lat'],\n",
       "  ['sheart', ' c1596', '1596', ['c'], 'lat'],\n",
       "  ['slid', ' 1606', '1606', [], 'lat'],\n",
       "  ['slife', ' a1634', '1634', ['a'], 'lat'],\n",
       "  ['slight', ' 1600', '1600', [], 'lat'],\n",
       "  ['slud', ' 1606', '1606', [], 'lat'],\n",
       "  ['snails', ' 1599', '1599', [], 'lat'],\n",
       "  ['sneaks', ' 1602', '1602', [], 'lat'],\n",
       "  ['sniggers', ' 1633', '1633', [], 'lat'],\n",
       "  ['snigs', ' a1643', '1643', ['a'], 'lat'],\n",
       "  ['snowns', ' 1594', '1594', [], 'lat'],\n",
       "  ['sprecious', ' 1631', '1631', [], 'lat'],\n",
       "  ['swill', ' 1602', '1602', [], 'lat'],\n",
       "  ['arf', ' 1854', '1854', [], 'neo'],\n",
       "  ['at', ' a1300', '1300', ['a'], 'lat']])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exceptions, len(terms_and_origins), terms_and_origins[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'germ': 7740, 'lat': 115336, 'neo': 126012})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([ i[4] for i in terms_and_origins])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#scrub repeated terms, keep one with lower date\n",
    "terms = {}\n",
    "for i in terms_and_origins:\n",
    "    try:\n",
    "        a = terms[i[0]]\n",
    "        if i[2] < a[2]:\n",
    "            terms[i[0]] = i    \n",
    "    except:\n",
    "        terms[i[0]] = i\n",
    "oed_normalized = terms.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ˌunder-ˈestimate', ' 1812', '1812', [], 'neo']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms['ˌunder-ˈestimate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247546"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\\\u02cc'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\"ˌ\").encode('unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
